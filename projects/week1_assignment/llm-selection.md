# LLM Selection

## Selection Criteria
- Open-source availability
- Ability to run locally
- Model performance
- Hardware requirements
- Community support

---

## Models Considered
| Model | Size | Pros | Cons |
|------|------|------|------|
| llama3:8b | Medium | High quality, strong reasoning | Higher memory usage |
| mistral:7b | Medium | Fast, efficient | Slightly less contextual depth |
| phi3:mini | Small | Low resource usage | Limited reasoning ability |

---

## Selected Model
**llama3:8b**

---

## Justification
- Balanced performance and quality
- Suitable for conversational and technical tasks
- Supported by Ollama for local execution
- Strong community adoption

---

## Alternative Model
**mistral:7b** was evaluated as a lightweight alternative for lower-memory systems.
